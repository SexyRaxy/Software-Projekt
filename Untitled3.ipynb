{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from somajo import SoMaJo\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/weltk.JOEL-PC/Software Projekt/germevalfinal.txt\", encoding=\"utf-8-sig\") as tweets, open(\"/Users/weltk.JOEL-PC/Software Projekt/schlist.txt\", encoding=\"utf-8-sig\") as sw, open (\"/Users/weltk.JOEL-PC/Software Projekt/gesternt.txt\") as gesternt:\n",
    "    data = tweets.read()\n",
    "    schlist = sw.read()\n",
    "    schimpf= gesternt.read()\n",
    "    \n",
    "data_remove = data.replace(\"OTHER\", \"0\")\n",
    "data_remove_2 = data_remove.replace(\"OFFENSE\", \"1\")\n",
    "data_remove_3 = data_remove_2.replace(\"INSULT\", \"\")\n",
    "data_remove_4 = data_remove_3.replace(\"ABUSE\", \"\")\n",
    "data_neu = data_remove_4.replace(\"PROFANITY\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "#Implementation der deutschen Stoppwortliste via NLTK\n",
    "stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "\n",
    "#Erweitern der Liste\n",
    "other_exclusions = [\"rt\", \"aber\", \"alle\", \"allem\", \"allen\", \"aller\", \"alles\", \"als\", \"also\", \"am\", \"an\", \"ander\", \"andere\", \"anderem\", \"anderen\", \"anderer\", \"anderes\", \"anderm\", \"andern\", \"anderr\", \n",
    "                  \"anders\", \"auch\", \"auf\", 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', \n",
    "                  'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies',\n",
    "                  'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges',\n",
    "                  'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin',\n",
    "                  'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem',\n",
    "                  'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', \n",
    "                  'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', \n",
    "                  'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern',\n",
    "                  'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil',\n",
    "                  'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden',\n",
    "                  'zu', 'zum', 'zur', 'zwar', 'zwischen', \"ja\", \"mehr\", \"warum\", \"wäre\", \"geht\", \"mal\", \"wer\", \"macht\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "## 1. Entfernen von Satzzeichen, Sonderzeichen und lowercase transformation\n",
    "def preprocess(data_neu):  \n",
    "    \n",
    "    # entfernen von Extra Leerzeichen\n",
    "    \n",
    "    tweet_space = data_neu.replace(r'\\s+', ' ')\n",
    "\n",
    "    # Entfernen von Usernames @'s\n",
    "\n",
    "    tweet_name = tweet_space.replace(r'@[\\w\\-]+', '')\n",
    "\n",
    "    # Entfernen von URLS\n",
    "    \n",
    "    tweets = tweet_name.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '')\n",
    "    \n",
    "    # Entfernen von Satzzeichen\n",
    "    punc_remove = tweets.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # Entfernen von Leerzeichen\n",
    "    newtweet= punc_remove.replace(r'\\s+', ' ')\n",
    "    # Entfernen von Leerzeichen am Anfang des Tweets/Ende\n",
    "    newtweet= newtweet.replace(r'^\\s+|\\s+?$','')\n",
    "    # Ersetzen von Nummern mit \"numbr\"\n",
    "    newtweet= newtweet.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    # Entfernen von Großschreibung\n",
    "    tweet_lower = newtweet.lower()\n",
    "\n",
    "    \n",
    "## 2: Tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "## 3: Entfernen von Stoppwörtern\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "## 4: Stemmen der Tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "\n",
    "#Festlegen der Variable processed_tweets zum einfacheren Workflow\n",
    "\n",
    "\n",
    "## FEHLENDE VARIABLEN ÜBERGABE\n",
    "\n",
    "\n",
    "#Ausgabe der Preprocess Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "\n",
    "sentences = nltk.sent_tokenize(data_neu,language='german')\n",
    "\n",
    "tokenized_sent = nltk.tokenize.word_tokenize(sentences[5009],language='german')\n",
    "print(tokenized_sent)\n",
    "\n",
    "sentences = nltk.sent_tokenize(data_neu,language='german')\n",
    "\n",
    "tokenized_text = [nltk.word_tokenize(sent, language='german') for sent in sentences]\n",
    "\n",
    "print(tokenized_text[0])\n",
    "print(tokenized_text[5])\n",
    "\n",
    "#Tagger\n",
    "tags = nltk.pos_tag(tokenized_text[0])\n",
    "print(tags)\n",
    "\n",
    "#Lemmatisierung\n",
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "\n",
    "lemmata = [lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in tags]\n",
    "print(lemmata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gesternte Wörter finden\n",
    "exp = re.findall(r\"[A-Za-z]+[*][*]+\", str(data), re.MULTILINE)\n",
    "zensiert = re.findall(r\"[*]+[A-Za-z]+[*]+\", str(data), re.MULTILINE)\n",
    "\n",
    "#Somajo tokenisierung\n",
    "tokenizer = SoMaJo(\"de_CMC\")\n",
    "tweets = tokenizer.tokenize_text_file(\"/Users/weltk.JOEL-PC/Software Projekt/germevalfinal.txt\", paragraph_separator=\"single_newlines\")\n",
    "schimpfwort = tokenizer.tokenize_text_file(\"/Users/weltk.JOEL-PC/Software Projekt/schlist.txt\", paragraph_separator=\"single_newlines\")\n",
    "\n",
    "#zählen der zensierten Wörter\n",
    "target = []\n",
    "for item in exp:\n",
    "    target.append(item[0])\n",
    "print(len(set(target)))\n",
    "\n",
    "\n",
    "for i in exp:\n",
    "    laenge = len(i)\n",
    "    print(i + \" \" + str(laenge))\n",
    "    \n",
    "# Suche nach den zensierten Wörtern in der schimpfwortliste\n",
    "def stern(text):\n",
    "    for lines in schlist:\n",
    "        for line in text:\n",
    "             if lines == line:\n",
    "                print(\"Das zensierte Wort ähnelt:\", line)\n",
    "                #re.sub(schimpf, \"OFFENSIVE\" ,data)\n",
    "print(stern(schimpf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Großgeschriebene Wörter finden\n",
    "gross = re.findall(r\"\\s[A-Z][A-Z]+\", str(data_neu))\n",
    "#Hashtags suchen\n",
    "hashtags = re.findall(r\"#[A-Za-z]+\", str(data_neu))\n",
    "\n",
    "# Aufgabe2: Wörter in groß und schlist klein schreiben\n",
    "schlist_lower = [each_string.lower() for each_string in schlist]\n",
    "gross_lower = [every_string.lower() for every_string in gross]\n",
    "hashtags_lower = [every_string.lower() for every_string in hashtags]\n",
    "\n",
    "#große Wörter in Schimpfwortliste finden\n",
    "#Problem: vergleich auch einzelne Buchstaben\n",
    "def caps(daten):\n",
    "    for line in daten:\n",
    "        for lines in gross_lower:\n",
    "            if line in lines:\n",
    "                print(\"Dieses Wort ist offensive:\", line)\n",
    "#print(caps(schlist_lower).head(10))\n",
    "\n",
    "\n",
    "#Alle Hashtags in der Schimpfwortliste finden\n",
    "def ht(daten):\n",
    "    for line in daten:\n",
    "        for lines in hashtags_lower:\n",
    "            if line in lines:\n",
    "                print(\"Dieses Wort ist offensive:\", line)\n",
    "#print(ht(schlist_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
