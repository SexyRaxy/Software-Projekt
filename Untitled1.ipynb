{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import seaborn as sns\n",
    "from somajo import SoMaJo\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der gesamten Trainingsdaten \n",
    "df = panda.read_csv(\"test20192.csv\", sep=\";\")\n",
    "df.head()\n",
    "\n",
    "# anlegen der Variablen \"tweet\" & \"classi\"\n",
    "tweet = df[\"tweet\"]\n",
    "classi = df[\"classification\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testdaten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = panda.read_csv(\"test201922.csv\", sep=\";\")\n",
    "test.head()\n",
    "\n",
    "tweet2 = test[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  class  classification  \\\n",
      "0  @jouwatch Hat die Polizei keine Kanone mehr ? ...    1.0               1   \n",
      "1  @de_sputnik @Saudi Arabien habt ihr mal wieder...    1.0               1   \n",
      "2  Glaube ich nicht , die Bundesregierung so wie ...    1.0               1   \n",
      "3   Doch schockierend viele Jugendliche wissen ka...    1.0               1   \n",
      "4  Sein Charakter war ihm wichtiger anstatt als b...    1.0               1   \n",
      "\n",
      "   text length  \n",
      "0           97  \n",
      "1          255  \n",
      "2          147  \n",
      "3          272  \n",
      "4          122  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1ed61aab460>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hinzufügen der Spalte \"Text length\" und berechnen der Tweetlänge\n",
    "df['text length'] = df['tweet'].apply(len)\n",
    "print(df.head())\n",
    "\n",
    "#Visualisierung der Textlänge nach Klassifizierung\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "graph = sns.FacetGrid(data=df, col='classification')\n",
    "graph.map(plt.hist, 'text length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SoMaJo(\"de_CMC\", split_camel_case=True)\n",
    "schimpfwoerter = tokenizer.tokenize_text_file(\"schlist.txt\", paragraph_separator=\"single_newlines\")\n",
    "#toktweets = tokenizer.tokenize_text_file(\"germeval2018.training.txt\", paragraph_separator=\"single_newlines\")\n",
    "\n",
    "\n",
    "# Tokenisierung\n",
    "def tokenize(text):\n",
    "    textli = []\n",
    "    for sentence in text:\n",
    "        for token in sentence:\n",
    "            y = token.text\n",
    "            textli.append(y)\n",
    "    return textli\n",
    "\n",
    "\n",
    "# tokenisierte Schimpfwortliste\n",
    "schlist = tokenize(schimpfwoerter)\n",
    "#tweets = tokenize(toktweets)\n",
    "schlist_lower = [each_string.lower() for each_string in schlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0  @jouwatch Hat die Polizei keine Kanone mehr ? ...   \n",
      "1  @de_sputnik @Saudi Arabien habt ihr mal wieder...   \n",
      "2  Glaube ich nicht , die Bundesregierung so wie ...   \n",
      "3   Doch schockierend viele Jugendliche wissen ka...   \n",
      "4  Sein Charakter war ihm wichtiger anstatt als b...   \n",
      "5  @de_sputnik @JPN_PMO \" Obwohl das US-Militär g...   \n",
      "6  Warum schalten sich die verdammten Schmierenko...   \n",
      "7  Ob Jesus für das oder jenes Starb ist doch woh...   \n",
      "8  @de_sputnik Ob Jesus für das oder jenes Starb ...   \n",
      "9  Das ist allerdings wahr , jeder Amerikaner wei...   \n",
      "\n",
      "                                         clean_tweet  \n",
      "0  hat die polizei keine kanone mehr numbr mal in...  \n",
      "1  arabien habt ihr mal wieder einen orgasmus geb...  \n",
      "2  glaube ich nicht die bundesregierung so wie di...  \n",
      "3  doch schockierend viele jugendliche wissen kau...  \n",
      "4  sein charakter war ihm wichtiger anstatt als b...  \n",
      "5  obwohl das us militär gemäß dem sicherheitsver...  \n",
      "6  warum schalten sich die verdammten schmierenko...  \n",
      "7  ob jesus für das oder jenes starb ist doch woh...  \n",
      "8  ob jesus für das oder jenes starb ist doch woh...  \n",
      "9  das ist allerdings wahr jeder amerikaner weiß ...  \n"
     ]
    }
   ],
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "## 1. Entfernen von Satzzeichen, Sonderzeichen und lowercase transformation\n",
    "def preprocess(tweet):  \n",
    "    \n",
    "    # entfernen von Extra Leerzeichen\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # Entfernen von Usernames @'s\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # Entfernen von URLS\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # Entfernen von Satzzeichen\n",
    "    punc_remove = tweets.str.replace(r'[^\\w\\s^*]', \" \")\n",
    "    # Entfernen von Leerzeichen\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ')\n",
    "    # Entfernen von Leerzeichen am Anfang des Tweets/Ende\n",
    "    space_remove=newtweet.str.replace(r'^\\s+|\\s+?$','')\n",
    "    # Ersetzen von Nummern mit \"numbr\"\n",
    "    nmbr_remove = space_remove.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    # Entfernen von Großschreibung\n",
    "    tweet_lower = nmbr_remove.str.lower()\n",
    "    \n",
    "## 2: Tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "    \n",
    "    \n",
    "\n",
    "#Festlegen der Variable clean_tweets zum einfacheren Workflow\n",
    "clean_tweet = preprocess(tweet)   \n",
    "df['clean_tweet'] = preprocess(tweet)\n",
    "\n",
    "#Ausgabe der Preprocessing Ergebnisse\n",
    "print(df[[\"tweet\",\"clean_tweet\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet2 = preprocess(tweet2)\n",
    "test['clean_tweet'] = preprocess(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>classification</th>\n",
       "      <th>text length</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@jouwatch Hat die Polizei keine Kanone mehr ? ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>polizei kanone numbr munition laufen lassen ruhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@de_sputnik @Saudi Arabien habt ihr mal wieder...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>arabien habt orgasmus gebaucht weswegen habt f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glaube ich nicht , die Bundesregierung so wie ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>glaube bundesregierung justiz korrupt arbeiten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doch schockierend viele Jugendliche wissen ka...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>schockierend viele jugendliche wissen kaum hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sein Charakter war ihm wichtiger anstatt als b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>charakter wichtiger anstatt billige nute korru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class  classification  \\\n",
       "0  @jouwatch Hat die Polizei keine Kanone mehr ? ...    1.0               1   \n",
       "1  @de_sputnik @Saudi Arabien habt ihr mal wieder...    1.0               1   \n",
       "2  Glaube ich nicht , die Bundesregierung so wie ...    1.0               1   \n",
       "3   Doch schockierend viele Jugendliche wissen ka...    1.0               1   \n",
       "4  Sein Charakter war ihm wichtiger anstatt als b...    1.0               1   \n",
       "\n",
       "   text length                                        clean_tweet  \n",
       "0           97   polizei kanone numbr munition laufen lassen ruhe  \n",
       "1          255  arabien habt orgasmus gebaucht weswegen habt f...  \n",
       "2          147  glaube bundesregierung justiz korrupt arbeiten...  \n",
       "3          272  schockierend viele jugendliche wissen kaum hol...  \n",
       "4          122  charakter wichtiger anstatt billige nute korru...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3: Implementation der deutschen Stoppwortliste via NLTK\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"german\")\n",
    "\n",
    "#Erweitern der Liste\n",
    "other_exclusions = [\"rt\", \"ff\", \"#ff\", \"aber\", \"alle\", \"allem\", \"allen\", \"aller\", \"alles\", \"als\", \"also\", \"am\", \"an\", \"ander\", \"andere\", \"anderem\", \"anderen\", \"anderer\", \"anderes\", \"anderm\", \"andern\", \"anderr\", \n",
    "                  \"anders\", \"auch\", \"auf\", 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', \n",
    "                  'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies',\n",
    "                  'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges',\n",
    "                  'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin',\n",
    "                  'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem',\n",
    "                  'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', \n",
    "                  'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', \n",
    "                  'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern',\n",
    "                  'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil',\n",
    "                  'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden',\n",
    "                  'zu', 'zum', 'zur', 'zwar', 'zwischen', \"ja\", \"mehr\", \"warum\", \"wäre\", \"geht\", \"mal\", \"wer\", \"macht\",\"lbr\", \"schon\", \"immer\", \"heute\", \"ganz\", \"einfach\", \"gut\", \"seit\", \"wirklich\", \"beim\", \"ab\", \"dafür\", \"u\", \"d\", \"s\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word in set(stopwords)]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['clean_tweet'] = test['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word in set(stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995, 1224)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=6, max_features=10000)\n",
    "bow = bow_vectorizer.fit_transform(df['clean_tweet'])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3031, 967)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testbow = bow_vectorizer.fit_transform(test['clean_tweet'])\n",
    "testbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3995x1300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21089 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigrams & Bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.9, min_df=6, max_features=10000)\n",
    "\n",
    "# TF-IDF feature matrix erstellen\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['clean_tweet'] )\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3031x1063 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17289 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidftest = tfidf_vectorizer.fit_transform(test['clean_tweet'] )\n",
    "tfidftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608804, 827720)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, # gewünschte Anzahl von features/variablen\n",
    "            window=5, # window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1=skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # für negatives sampling\n",
    "            workers= 2, # Anzahl der arbeitenden Cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(df['clean_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495514, 694100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test = test['clean_tweet'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "test_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_test,\n",
    "            size=200, # gewünschte Anzahl von features/variablen\n",
    "            window=5, # window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1=skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # für negatives sampling\n",
    "            workers= 2, # Anzahl der arbeitenden Cores\n",
    "            seed = 34)\n",
    "\n",
    "test_w2v.train(tokenized_test, total_examples= len(test['clean_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # Absicherung, falls Token nicht im Vokabular\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += test_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # Absicherung, falls Token nicht im Vokabular\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-4558bf377a91>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vec += model_w2v[word].reshape((1, size))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3995, 200)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shapen des Modells\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "\n",
    "for i in range(len(clean_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "    \n",
    "wordvec_df = panda.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-2ac9d8c84d7d>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vec += test_w2v[word].reshape((1, size))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3031, 200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shapen des Modells\n",
    "wordvectest_arrays = np.zeros((len(tokenized_test), 200))\n",
    "\n",
    "for i in range(len(clean_tweet2)):\n",
    "    wordvectest_arrays[i,:] = test_vector(tokenized_test[i], 200)\n",
    "    \n",
    "wordvec_test = panda.DataFrame(wordvectest_arrays)\n",
    "wordvec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-868d96c8c1ce>:4: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n"
     ]
    }
   ],
   "source": [
    "labeled_tweets = add_label(tokenized_tweet) # alle tweets werden gelabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-868d96c8c1ce>:4: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n"
     ]
    }
   ],
   "source": [
    "labeled_test = add_label(tokenized_test) # alle tweets werden gelabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "100%|██████████| 3995/3995 [00:00<00:00, 4003881.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model_d2v = gensim.models.Doc2Vec(dm=1, # 1=‘distributed memory’ model \n",
    "                                  dm_mean=1, # 1=Benutzung der Kontext Vektoren\n",
    "                                  size=200, # Anzahl der gewünschten Features\n",
    "                                  window=5, # Window size\n",
    "                                  negative=7, # wenn größer 7, Benutzung negativer sampler\n",
    "                                  min_count=5, # Ignoriert alle Wörter mit einer Frequency unter 5\n",
    "                                  workers=3, # Arbeitende Cores\n",
    "                                  alpha=0.1, # Lernrate\n",
    "                                  seed = 23)\n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3031/3031 [00:00<00:00, 3039190.87it/s]\n"
     ]
    }
   ],
   "source": [
    "test_d2v = gensim.models.Doc2Vec(dm=1, # 1=‘distributed memory’ model \n",
    "                                  dm_mean=1, # 1=Benutzung der Kontext Vektoren\n",
    "                                  size=200, # Anzahl der gewünschten Features\n",
    "                                  window=5, # Window size\n",
    "                                  negative=7, # wenn größer 7, Benutzung negativer sampler\n",
    "                                  min_count=5, # Ignoriert alle Wörter mit einer Frequency unter 5\n",
    "                                  workers=3, # Arbeitende Cores\n",
    "                                  alpha=0.1, # Lernrate\n",
    "                                  seed = 23)\n",
    "\n",
    "test_d2v.build_vocab([i for i in tqdm(labeled_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v.train(labeled_tweets, total_examples= len(df['clean_tweet']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d2v.train(labeled_test, total_examples= len(test['clean_tweet']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995, 200)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "\n",
    "for i in range(len(df)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
    "    \n",
    "docvec_df = panda.DataFrame(docvec_arrays)\n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3031, 200)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvectest_arrays = np.zeros((len(tokenized_test), 200))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    docvectest_arrays[i,:] = test_d2v.docvecs[i].reshape((1,200))\n",
    "    \n",
    "docvec_test = panda.DataFrame(docvectest_arrays)\n",
    "docvec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Compound</th>\n",
       "      <th>url_tag</th>\n",
       "      <th>mention_tag</th>\n",
       "      <th>hash_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.358</td>\n",
       "      <td>0.642</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.907</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262</td>\n",
       "      <td>0.738</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.907</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178</td>\n",
       "      <td>0.822</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>0.162</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3995 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Neg  Neutral  Compound  url_tag  mention_tag  hash_tag\n",
       "0     0.358    0.642   -0.8316      0.0          0.0       0.0\n",
       "1     0.093    0.907   -0.5994      0.0          0.0       0.0\n",
       "2     0.262    0.738   -0.8316      0.0          0.0       0.0\n",
       "3     0.093    0.907   -0.5994      0.0          0.0       0.0\n",
       "4     0.178    0.822   -0.5994      0.0          0.0       0.0\n",
       "...     ...      ...       ...      ...          ...       ...\n",
       "3990  0.000    0.928    0.1027      0.0          0.0       0.0\n",
       "3991  0.000    1.000    0.0000      0.0          0.0       0.0\n",
       "3992  0.162    0.664    0.0772      0.0          0.0       0.0\n",
       "3993  0.000    1.000    0.0000      0.0          0.0       0.0\n",
       "3994  0.000    1.000    0.0000      0.0          0.0       0.0\n",
       "\n",
       "[3995 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STARTEN DER SENTIMENT ANALYSE\n",
    "#nltk.download('vader_lexicon')\n",
    "sentiment_analyzer = VS()\n",
    "#Auszählen der Tags und bewerten nach Klassifizierung\n",
    "def count_tags(tweet_c): \n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', tweet_c)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    \n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    \n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    \n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "# definieren von 3 Segementen \n",
    "def sentiment_analysis(clean_tweet):   \n",
    "    sentiment = sentiment_analyzer.polarity_scores(clean_tweet)    \n",
    "    twitter_objs = count_tags(clean_tweet)\n",
    "    features = [sentiment['neg'], sentiment['neu'], sentiment['compound'], twitter_objs[0], twitter_objs[1],\n",
    "                twitter_objs[2]]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def sentiment_analysis_array(clean_tweet):\n",
    "    features=[]\n",
    "    for t in clean_tweet:\n",
    "        features.append(sentiment_analysis(t))\n",
    "    return np.array(features)\n",
    "\n",
    "final_features = sentiment_analysis_array(clean_tweet)\n",
    "#final_features\n",
    "\n",
    "# Negativ = vorkommen vom geparsten Text in OFFENSIVE, Neutral = vorkommen vom Geparsten Text in OTHER \n",
    "new_features = panda.DataFrame({'Neg':final_features[:,0],'Neutral':final_features[:,1],'Compound':final_features[:,2],\n",
    "                            'url_tag':final_features[:,3],'mention_tag':final_features[:,4],'hash_tag':final_features[:,5]})\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Compound</th>\n",
       "      <th>url_tag</th>\n",
       "      <th>mention_tag</th>\n",
       "      <th>hash_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.245</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262</td>\n",
       "      <td>0.738</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.9136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>0.170</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>0.245</td>\n",
       "      <td>0.755</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>0.151</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>0.119</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3031 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Neg  Neutral  Compound  url_tag  mention_tag  hash_tag\n",
       "0     0.245    0.755   -0.5994      0.0          0.0       0.0\n",
       "1     0.000    1.000    0.0000      0.0          0.0       0.0\n",
       "2     0.262    0.738   -0.5994      0.0          0.0       0.0\n",
       "3     0.119    0.881   -0.5994      0.0          0.0       0.0\n",
       "4     0.302    0.698   -0.9136      0.0          0.0       0.0\n",
       "...     ...      ...       ...      ...          ...       ...\n",
       "3026  0.100    0.900   -0.5994      0.0          0.0       0.0\n",
       "3027  0.170    0.830   -0.8316      0.0          0.0       0.0\n",
       "3028  0.245    0.755   -0.8316      0.0          0.0       0.0\n",
       "3029  0.151    0.849   -0.5994      0.0          0.0       0.0\n",
       "3030  0.119    0.881   -0.5994      0.0          0.0       0.0\n",
       "\n",
       "[3031 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features2 = sentiment_analysis_array(clean_tweet2)\n",
    "#final_features\n",
    "new_features2 = panda.DataFrame({'Neg':final_features2[:,0],'Neutral':final_features2[:,1],'Compound':final_features2[:,2],\n",
    "                            'url_tag':final_features2[:,3],'mention_tag':final_features2[:,4],'hash_tag':final_features2[:,5]})\n",
    "new_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6049382716049383"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "\n",
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, df['classification'],  \n",
    "                                                          random_state=42, \n",
    "                                                          test_size=0.1)\n",
    "\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain) # trainieren des Modells\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # Beachtung nur, wenn Prediction > 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int) # Kalkulation vom f1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.592391304347826"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf2 = tfidftest[31962:,:]\n",
    "\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lreg.predict_proba(tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5564304461942257"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w2v = wordvec_df.iloc[:31962,:]\n",
    "test_w2v = wordvec_df.iloc[31962:,:]\n",
    "\n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "\n",
    "lreg.fit(xtrain_w2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_w2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5474254742547425"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d2v = docvec_df.iloc[:31962,:]\n",
    "test_d2v = docvec_df.iloc[31962:,:]\n",
    "\n",
    "xtrain_d2v = train_d2v.iloc[ytrain.index,:]\n",
    "xvalid_d2v = train_d2v.iloc[yvalid.index,:]\n",
    "\n",
    "lreg.fit(xtrain_d2v, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_d2v)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995, 1306)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF & Sentiment Analyse verknüpfen\n",
    "tfidf_a = tfidf.toarray()\n",
    "modelling_features = np.concatenate([tfidf_a,final_features],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3031, 1069)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF & Sentiment Analyse verknüpfen\n",
    "tfidf_b = tfidftest.toarray()\n",
    "modelling_test = np.concatenate([tfidf_b,final_features2],axis=1)\n",
    "modelling_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90      2708\n",
      "           1       0.85      0.71      0.77      1287\n",
      "\n",
      "    accuracy                           0.87      3995\n",
      "   macro avg       0.86      0.82      0.84      3995\n",
      "weighted avg       0.86      0.87      0.86      3995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression mit TF-IDF & Sentiment Analyse\n",
    "\n",
    "x = panda.DataFrame(modelling_features)\n",
    "y = df['classification'].astype(int)\n",
    "\n",
    "# Erstellen & trainieren des Modells\n",
    "model = LogisticRegression(solver='liblinear', C=30, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluation\n",
    "p_pred = model.predict_proba(x)\n",
    "y_pred = model.predict(x)\n",
    "score_ = model.score(x, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      2061\n",
      "           1       0.88      0.74      0.80       970\n",
      "\n",
      "    accuracy                           0.88      3031\n",
      "   macro avg       0.88      0.85      0.86      3031\n",
      "weighted avg       0.88      0.88      0.88      3031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_x = panda.DataFrame(modelling_test)\n",
    "test_y = test['classification'].astype(int)\n",
    "\n",
    "model.fit(test_x,test_y)\n",
    "test_pred = model.predict(test_x)\n",
    "testreport = classification_report(test_y, test_pred)\n",
    "print(testreport)\n",
    "\n",
    "#test_predictions = model.predict(test_x)\n",
    "#f1_score(test_y, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995, 1506)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verketten des TF-IDF & Sentiment Scores mit den Word2Vec Ergebnissen\n",
    "modelling_features1 = np.concatenate([tfidf_a,final_features,wordvec_df],axis=1)\n",
    "modelling_features1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3031, 1269)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verketten des TF-IDF & Sentiment Scores mit den Word2Vec Ergebnissen\n",
    "modelling_test1 = np.concatenate([tfidf_b,final_features2,wordvec_test],axis=1)\n",
    "modelling_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91      2708\n",
      "           1       0.86      0.72      0.78      1287\n",
      "\n",
      "    accuracy                           0.87      3995\n",
      "   macro avg       0.87      0.83      0.85      3995\n",
      "weighted avg       0.87      0.87      0.87      3995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression mit TF-IDF, Sentiment Analyse & Word2Vec\n",
    "\n",
    "x = panda.DataFrame(modelling_features1)\n",
    "y = df['classification'].astype(int)\n",
    "\n",
    "# Erstellen & trainieren des Modells\n",
    "model = LogisticRegression(solver='liblinear', C=30, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluation\n",
    "p_pred = model.predict_proba(x)\n",
    "y_pred = model.predict(x)\n",
    "score_ = model.score(x, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92      2061\n",
      "           1       0.89      0.76      0.82       970\n",
      "\n",
      "    accuracy                           0.89      3031\n",
      "   macro avg       0.89      0.86      0.87      3031\n",
      "weighted avg       0.89      0.89      0.89      3031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_x = panda.DataFrame(modelling_test1)\n",
    "test_y = test['classification'].astype(int)\n",
    "\n",
    "model.fit(test_x,test_y)\n",
    "test_pred = model.predict(test_x)\n",
    "testreport = classification_report(test_y, test_pred)\n",
    "print(testreport)\n",
    "\n",
    "#test_predictions = model.predict(test_x)\n",
    "#f1_score(test_y, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995, 1706)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verketten der TF-IDF, Sentiment Analyse, Word2Vec & Doc2Vec\n",
    "modelling_features2 = np.concatenate([tfidf_a,final_features,wordvec_df,docvec_df],axis=1)\n",
    "modelling_features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3031, 1469)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verketten der TF-IDF, Sentiment Analyse, Word2Vec & Doc2Vec\n",
    "modelling_test2 = np.concatenate([tfidf_b,final_features2,wordvec_test,docvec_test],axis=1)\n",
    "modelling_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      2708\n",
      "           1       0.87      0.73      0.79      1287\n",
      "\n",
      "    accuracy                           0.88      3995\n",
      "   macro avg       0.87      0.84      0.85      3995\n",
      "weighted avg       0.88      0.88      0.87      3995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression mit TF-IDF, Sentiment Analyse, Word2Vec & Doc2Vec\n",
    "\n",
    "x = panda.DataFrame(modelling_features2)\n",
    "y = df['classification'].astype(int)\n",
    "\n",
    "# Erstellen & trainieren des Modells\n",
    "model = LogisticRegression(solver='liblinear', C=30, random_state=0)\n",
    "model.fit(x, y)\n",
    "\n",
    "# Evaluation\n",
    "p_pred = model.predict_proba(x)\n",
    "y_pred = model.predict(x)\n",
    "score_ = model.score(x, y)\n",
    "conf_m = confusion_matrix(y, y_pred)\n",
    "report = classification_report(y, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93      2061\n",
      "           1       0.90      0.77      0.83       970\n",
      "\n",
      "    accuracy                           0.90      3031\n",
      "   macro avg       0.90      0.86      0.88      3031\n",
      "weighted avg       0.90      0.90      0.90      3031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_x = panda.DataFrame(modelling_test2)\n",
    "test_y = test['classification'].astype(int)\n",
    "\n",
    "model.fit(test_x,test_y)\n",
    "test_pred = model.predict(test_x)\n",
    "testreport = classification_report(test_y, test_pred)\n",
    "print(testreport)\n",
    "\n",
    "#test_predictions = model.predict(test_x)\n",
    "#f1_score(test_y, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
